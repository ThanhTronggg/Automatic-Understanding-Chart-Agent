{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:09:09.343602Z",
     "iopub.status.busy": "2024-12-17T15:09:09.343008Z",
     "iopub.status.idle": "2024-12-17T15:09:09.533925Z",
     "shell.execute_reply": "2024-12-17T15:09:09.533031Z",
     "shell.execute_reply.started": "2024-12-17T15:09:09.343569Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34.2k/34.2k [00:00<00:00, 15.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.hub.download_url_to_file(\n",
    "    'https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/multi_col_1229.png', \n",
    "    'chart_example_1.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChartInstruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:44:09.228122Z",
     "iopub.status.busy": "2024-12-17T14:44:09.227194Z",
     "iopub.status.idle": "2024-12-17T14:44:36.918077Z",
     "shell.execute_reply": "2024-12-17T14:44:36.917169Z",
     "shell.execute_reply.started": "2024-12-17T14:44:09.228086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d294761ff3439eb1dc824111b60408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n",
      "Expanding inputs for image tokens in LLaVa should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Output: 25% \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model = LlavaForConditionalGeneration.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\", torch_dtype=torch.float16)\n",
    "processor = AutoProcessor.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "image_path = \"chart_example_1.png\"\n",
    "input_text = \"What is the share of respondants who prefer Whatsapp in the 30-59 age group?\"\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "input_prompt = f\"<image>\\n Question: {input_text} Answer: \"\n",
    "\n",
    "inputs = processor(text=input_prompt, images=image, return_tensors=\"pt\")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
    "\n",
    "prompt_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "generate_ids = model.generate(**inputs, num_beams=4, max_new_tokens=512)\n",
    "output_text = processor.batch_decode(generate_ids[:, prompt_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "print(\"Model Output:\", output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart-to-Table Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chart-To-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:44:36.920050Z",
     "iopub.status.busy": "2024-12-17T14:44:36.919392Z",
     "iopub.status.idle": "2024-12-17T14:45:19.281684Z",
     "shell.execute_reply": "2024-12-17T14:45:19.280769Z",
     "shell.execute_reply.started": "2024-12-17T14:44:36.920020Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f95617294b549dd87b790a523fdcf7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31068b305afd4369966221616c1045e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/809M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    960,\n",
      "    960\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 10\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57531\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf66e3e9a4d43cb80aa358614bf5a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bac43700ba9e4c998b3a2483a5783f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/439 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09559e66fa164a22ace4813b84e005f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/510 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18c18574f454d618f9eb18d3d687dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/1.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25be5808d2e64f7daaa731094e43a974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.01M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241b5b501c2a417b92668c2ce3988d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/235 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13651cad9a0140e1a3b9cc5746552c4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/355 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TITLE | Share of Facebook Messenger users in the United States as of January 2018, by age group &&& Characteristic | Facebook Messenger | WhatsApp &&& 18-29 | 73% | 30% &&& 30-59 | 66% | 25% &&& 60+ | 43% | 6%'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "model_name = \"khhuang/chart-to-table\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name).cuda()\n",
    "processor = DonutProcessor.from_pretrained(model_name)\n",
    "\n",
    "image_path = \"chart_example_1.png\"\n",
    "\n",
    "input_prompt = \"<data_table_generation> <s_answer>\"\n",
    "\n",
    "img = Image.open(image_path)\n",
    "pixel_values = processor(img.convert(\"RGB\"), random_padding=False, return_tensors=\"pt\").pixel_values\n",
    "pixel_values = pixel_values.cuda()\n",
    "decoder_input_ids = processor.tokenizer(input_prompt, add_special_tokens=False, return_tensors=\"pt\", max_length=510).input_ids.cuda()#.squeeze(0)\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "        pixel_values.cuda(),\n",
    "        decoder_input_ids=decoder_input_ids.cuda(),\n",
    "        max_length=model.decoder.config.max_position_embeddings,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        use_cache=True,\n",
    "        num_beams=4,\n",
    "        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n",
    "        return_dict_in_generate=True,\n",
    "    )\n",
    "    \n",
    "\n",
    "sequence = processor.batch_decode(outputs.sequences)[0]\n",
    "sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n",
    "extracted_table = sequence.split(\"<s_answer>\")[1].strip()\n",
    "extracted_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:45:19.283623Z",
     "iopub.status.busy": "2024-12-17T14:45:19.283171Z",
     "iopub.status.idle": "2024-12-17T14:45:19.308548Z",
     "shell.execute_reply": "2024-12-17T14:45:19.307741Z",
     "shell.execute_reply.started": "2024-12-17T14:45:19.283582Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Characteristic</th>\n",
       "      <th>Facebook Messenger</th>\n",
       "      <th>WhatsApp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18-29</td>\n",
       "      <td>73%</td>\n",
       "      <td>30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-59</td>\n",
       "      <td>66%</td>\n",
       "      <td>25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60+</td>\n",
       "      <td>43%</td>\n",
       "      <td>6%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Characteristic Facebook Messenger WhatsApp \n",
       "0           18-29                73%      30% \n",
       "1           30-59                66%      25% \n",
       "2             60+                43%        6%"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_string = extracted_table.replace('&&&', '\\n')\n",
    "lines = data_string.split('\\n')\n",
    "\n",
    "has_title = \"TITLE\" in data_string\n",
    "\n",
    "header_lines = [line.split(' | ') for line in lines if '|' in line][:2 if has_title else 1]\n",
    "\n",
    "header_length = len(header_lines[-1])\n",
    "lines = lines[2:] if has_title else lines[1:]\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        parts = line.split(' | ')\n",
    "        data.append(parts)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(data, columns=header_lines[-1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DePlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:45:19.311350Z",
     "iopub.status.busy": "2024-12-17T14:45:19.310659Z",
     "iopub.status.idle": "2024-12-17T14:46:05.315414Z",
     "shell.execute_reply": "2024-12-17T14:46:05.314380Z",
     "shell.execute_reply.started": "2024-12-17T14:45:19.311312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0042041a38754f75944b6338207fb56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/249 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70a3ea567f14cdbb7b47ed9007a06bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea6e5a3eda94af48ae9a57e815017cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/851k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e557cc542c6448f1b0d54b97b209b256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214b5965833d4cf29818414a30641efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57607de0d5c04623ba8901677924e901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.88k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42f413c129624492b7682f55a9cd1bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf47a78cafa4eb18ac6774ea9f63381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Arial.TTF:   0%|          | 0.00/276k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characteristic | Facebook Messenger | WhatsApp <0x0A> 18-29 | 73% | 30% <0x0A> 30-59 | 66% | 25% <0x0A> 60+ | 43% | 6%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Characteristic</th>\n",
       "      <th>Facebook Messenger</th>\n",
       "      <th>WhatsApp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18-29</td>\n",
       "      <td>73%</td>\n",
       "      <td>30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-59</td>\n",
       "      <td>66%</td>\n",
       "      <td>25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60+</td>\n",
       "      <td>43%</td>\n",
       "      <td>6%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Characteristic Facebook Messenger WhatsApp \n",
       "0          18-29                73%      30% \n",
       "1          30-59                66%      25% \n",
       "2            60+                43%        6%"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
    "import pandas as pd\n",
    "\n",
    "image = Image.open(r'chart_example_1.png')\n",
    "\n",
    "processor = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n",
    "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\")\n",
    "\n",
    "description = \"Generate underlying data table of the figure below:\"\n",
    "\n",
    "encoding = processor(image, description, return_tensors=\"pt\")\n",
    "\n",
    "generated_ids = model.generate(**encoding, max_new_tokens=512)\n",
    "data_string = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(data_string)\n",
    "\n",
    "data_string = data_string.replace('<0x0A>', '\\n')\n",
    "lines = data_string.split('\\n')\n",
    "\n",
    "has_title = \"TITLE\" in data_string\n",
    "\n",
    "header_lines = [line.split(' | ') for line in lines if '|' in line][:2 if has_title else 1]\n",
    "\n",
    "header_length = len(header_lines[-1])\n",
    "lines = lines[2:] if has_title else lines[1:]\n",
    "\n",
    "data = []\n",
    "for line in lines:\n",
    "    try:\n",
    "        parts = line.split(' | ')\n",
    "        data.append(parts)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "df = pd.DataFrame(data, columns=header_lines[-1])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chart Fact-Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChartVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:46:05.316896Z",
     "iopub.status.busy": "2024-12-17T14:46:05.316586Z",
     "iopub.status.idle": "2024-12-17T14:46:12.462529Z",
     "shell.execute_reply": "2024-12-17T14:46:12.461588Z",
     "shell.execute_reply.started": "2024-12-17T14:46:05.316866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    14,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"image_size\": [\n",
      "    960,\n",
      "    960\n",
      "  ],\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"donut-swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"qkv_bias\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 10\n",
      "}\n",
      "\n",
      "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_cross_attention\": true,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 4,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_decoder\": true,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 1536,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"transformers_version\": \"4.46.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 57531\n",
      "}\n",
      "\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/tmp/ipykernel_23/2903374579.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  binary_entail_prob_positive = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1].item()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9958046078681946"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DonutProcessor, VisionEncoderDecoderModel\n",
    "from PIL import Image\n",
    "\n",
    "model_name = \"khhuang/chartve\"\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_name).cuda()\n",
    "processor = DonutProcessor.from_pretrained(model_name)\n",
    "\n",
    "image_path = \"chart_example_1.png\"\n",
    "\n",
    "def format_query(sentence):\n",
    "    return f\"Does the image entails this statement: \\\"{sentence}\\\"?\"\n",
    "\n",
    "# Format text inputs\n",
    "CAPTION_SENTENCE = \"Share of Facebook Messenger users in the United States as of January 2018, by age group.\"\n",
    "query = format_query(CAPTION_SENTENCE)\n",
    "\n",
    "# Encode chart figure and tokenize text\n",
    "img = Image.open(\"chart_example_1.png\")\n",
    "pixel_values = processor(img.convert(\"RGB\"), random_padding=False, return_tensors=\"pt\").pixel_values\n",
    "pixel_values = pixel_values.cuda()\n",
    "decoder_input_ids = processor.tokenizer(query, add_special_tokens=False, return_tensors=\"pt\", max_length=510).input_ids.cuda()\n",
    "\n",
    "\n",
    "outputs = model(pixel_values, decoder_input_ids=decoder_input_ids)\n",
    "\n",
    "# positive_logit = outputs['logits'].squeeze()[-1,49922]\n",
    "# negative_logit = outputs['logits'].squeeze()[-1,2334] \n",
    "\n",
    "# Probe the probability of generating \"yes\"\n",
    "binary_entail_prob_positive = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1].item()\n",
    "binary_entail_prob_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:05:59.069448Z",
     "iopub.status.busy": "2024-12-17T15:05:59.069122Z",
     "iopub.status.idle": "2024-12-17T15:06:28.754432Z",
     "shell.execute_reply": "2024-12-17T15:06:28.753420Z",
     "shell.execute_reply.started": "2024-12-17T15:05:59.069401Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/microsoft/autogen.git@0.2\n",
      "  Cloning https://github.com/microsoft/autogen.git (to revision 0.2) to /tmp/pip-req-build-aao2mkmf\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/autogen.git /tmp/pip-req-build-aao2mkmf\n",
      "  Running command git checkout -b 0.2 --track origin/0.2\n",
      "  Switched to a new branch '0.2'\n",
      "  Branch '0.2' set up to track remote branch '0.2' from 'origin'.\n",
      "  Resolved https://github.com/microsoft/autogen.git to commit 3b4c0170b058a3629bfc58a75e1bc22e2b9a29eb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting openai>=1.3 (from autogen-agentchat==0.2.40)\n",
      "  Downloading openai-1.57.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting diskcache (from autogen-agentchat==0.2.40)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from autogen-agentchat==0.2.40) (2.4.0)\n",
      "Collecting flaml (from autogen-agentchat==0.2.40)\n",
      "  Downloading FLAML-2.3.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from autogen-agentchat==0.2.40) (1.26.4)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (from autogen-agentchat==0.2.40) (1.0.1)\n",
      "Collecting tiktoken (from autogen-agentchat==0.2.40)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogen-agentchat==0.2.40) (2.10.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.10/site-packages (from autogen-agentchat==0.2.40) (7.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from autogen-agentchat==0.2.40) (21.3)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.3->autogen-agentchat==0.2.40) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.3->autogen-agentchat==0.2.40) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai>=1.3->autogen-agentchat==0.2.40) (0.27.0)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.3->autogen-agentchat==0.2.40)\n",
      "  Downloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>=1.3->autogen-agentchat==0.2.40) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai>=1.3->autogen-agentchat==0.2.40) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/conda/lib/python3.10/site-packages (from openai>=1.3->autogen-agentchat==0.2.40) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat==0.2.40) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=2.6.0,<3,>=1.10->autogen-agentchat==0.2.40) (2.27.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from docker->autogen-agentchat==0.2.40) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from docker->autogen-agentchat==0.2.40) (1.26.18)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->autogen-agentchat==0.2.40) (3.1.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->autogen-agentchat==0.2.40) (2024.5.15)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen-agentchat==0.2.40) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen-agentchat==0.2.40) (1.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat==0.2.40) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat==0.2.40) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen-agentchat==0.2.40) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.26.0->docker->autogen-agentchat==0.2.40) (3.3.2)\n",
      "Downloading openai-1.57.4-py3-none-any.whl (390 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.3/390.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading FLAML-2.3.2-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: autogen-agentchat\n",
      "  Building wheel for autogen-agentchat (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autogen-agentchat: filename=autogen_agentchat-0.2.40-py3-none-any.whl size=382279 sha256=919e70851dd046e52326892bb8e299911fe92ef12b4943254794d24628db7b9e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-_wq4vq4v/wheels/59/8a/18/2310172647a3fbcb967e6b6f8faacc3dc90393ca488f85db08\n",
      "Successfully built autogen-agentchat\n",
      "Installing collected packages: jiter, flaml, diskcache, tiktoken, openai, autogen-agentchat\n",
      "Successfully installed autogen-agentchat-0.2.40 diskcache-5.6.3 flaml-2.3.2 jiter-0.8.2 openai-1.57.4 tiktoken-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/microsoft/autogen.git@0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:20.379965Z",
     "iopub.status.busy": "2024-12-17T15:07:20.379612Z",
     "iopub.status.idle": "2024-12-17T15:07:31.875557Z",
     "shell.execute_reply": "2024-12-17T15:07:31.874767Z",
     "shell.execute_reply.started": "2024-12-17T15:07:20.379932Z"
    }
   },
   "outputs": [],
   "source": [
    "from autogen import (Agent, AssistantAgent, GroupChat, GroupChatManager, UserProxyAgent, config_list_from_json,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.877525Z",
     "iopub.status.busy": "2024-12-17T15:07:31.876937Z",
     "iopub.status.idle": "2024-12-17T15:07:31.881649Z",
     "shell.execute_reply": "2024-12-17T15:07:31.880766Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.877498Z"
    }
   },
   "outputs": [],
   "source": [
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"gemini-1.5-flash-latest\",\n",
    "        \"api_key\": \"YOUR_API_KEY\",\n",
    "        \"base_url\": \"https://generativelanguage.googleapis.com/v1beta\",\n",
    "        \"api_type\": \"custom\",\n",
    "        \"tags\": [\"gemini\", \"google\"]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boss (Agent đưa ra câu hỏi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.882901Z",
     "iopub.status.busy": "2024-12-17T15:07:31.882631Z",
     "iopub.status.idle": "2024-12-17T15:07:31.893826Z",
     "shell.execute_reply": "2024-12-17T15:07:31.893003Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.882877Z"
    }
   },
   "outputs": [],
   "source": [
    "boss = UserProxyAgent(\n",
    "    name=\"Boss\",\n",
    "    # human_input_mode=\"ALWAYS\",\n",
    "    system_message=\"The boss who ask questions and give tasks.\",\n",
    "    code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Planner (Agent phân loại câu hỏi đầu vào)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.895681Z",
     "iopub.status.busy": "2024-12-17T15:07:31.895386Z",
     "iopub.status.idle": "2024-12-17T15:07:31.912231Z",
     "shell.execute_reply": "2024-12-17T15:07:31.911653Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.895652Z"
    }
   },
   "outputs": [],
   "source": [
    "planner = AssistantAgent(\n",
    "    name=\"Planner\",\n",
    "    system_message=\"\"\"\n",
    "    You are a helpful AI assistant.\n",
    "    Your tasks involve working with charts to provide accurate and insightful responses. Use your analytical and language skills to classify input questions into the following tasks:\n",
    "\n",
    "    Answering Questions About Charts (Chart Question Answering):\n",
    "    This involves providing responses to questions related to the content of a chart.\n",
    "\n",
    "    Converting Charts to Tables (Chart-to-Table Conversion):\n",
    "    Extract data from charts and present it in a clear and structured tabular format. Ensure all values, labels, and metadata from the chart are accurately included in the table.\n",
    "\n",
    "    Verifying Chart Accuracy (Chart Fact-Checking):\n",
    "    This involves verifying whether a given statement aligns with the input chart.\n",
    "\n",
    "    (Important) I will only respond by classifying your input into one of these three tasks or providing an answer related to these tasks. If the message does not fall into any of the three categories, return None. Additional requests, such as asking for a chart or other unrelated information, will not be addressed.\n",
    "    Input message: {input_message}\n",
    "    \"\"\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAAgent (Agent trả lời các câu hỏi QA) (Chart Question Answering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.913371Z",
     "iopub.status.busy": "2024-12-17T15:07:31.913122Z",
     "iopub.status.idle": "2024-12-17T15:07:31.929800Z",
     "shell.execute_reply": "2024-12-17T15:07:31.929212Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.913347Z"
    }
   },
   "outputs": [],
   "source": [
    "qa_agent = AssistantAgent(\n",
    "    name=\"QAAgent\",\n",
    "    system_message=\"\"\"You are a senior Python engineer. Your task is to answer factual questions based on the data or chart images provided. You will generate Python code to analyze charts, images, or other formats to extract the answers. Here are the steps you need to follow:\n",
    "    1. Load the pre-trained model ahmed-masry/ChartInstruct-LLama2\n",
    "    2. When gathering information: preparing the device (GPU/CPU), processing the input image and question into tensors, generating the answer using beam search, decoding the output, and printing the result.\n",
    "    3. When solving the task: - Generate the necessary Python code to solve the task. model = LlavaForConditionalGeneration.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\", torch_dtype=torch.float16)\n",
    "processor = AutoProcessor.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\"). image = Image.open(image_path).convert(\"RGB\");input_prompt = f\"<image>\\n Question: {input_text} Answer: \";inputs = processor(text=input_prompt, images=image, return_tensors=\"pt\");inputs = {k: v.to(device) for k, v in inputs.items()};inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16);prompt_length = inputs['input_ids'].shape[1];generate_ids = model.generate(**inputs, num_beams=4, max_new_tokens=512);output_text = processor.batch_decode(generate_ids[:, prompt_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0] - Ensure the code is complete and ready to execute without requiring modifications from the user. Do not use raise, if-else, or try-catch to handle errors.\n",
    "    4. Error handling: - If an error occurs, provide an updated script to fix the issue. Make sure to handle edge cases or potential issues in the data processing.\n",
    "    5. Task completion: After completing the task and generating the required Python code. Your goal is to generate efficient Python code that leverages the google/deplot model to convert charts into tabular data. The code you generate should be complete, functional, and ready to execute. After generating the Python code, close the ```.\n",
    "    6. Respond with \"TERMINATE\" to indicate that everything is done.\n",
    "    \"\"\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    "    code_execution_config=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversionAgent (Agent chuyển đổi biểu đồ thành bảng) (Chart-to-Table Conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.930913Z",
     "iopub.status.busy": "2024-12-17T15:07:31.930658Z",
     "iopub.status.idle": "2024-12-17T15:07:31.946570Z",
     "shell.execute_reply": "2024-12-17T15:07:31.946003Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.930890Z"
    }
   },
   "outputs": [],
   "source": [
    "conversion_agent = AssistantAgent(\n",
    "    name=\"ConversionAgent\",\n",
    "    system_message=\"\"\"You are a senior Python engineer. Your task is to generate Python code that uses pre-trained models to analyze charts and generate underlying data. Specifically, you will use the google/deplot model to convert a chart into a data table. Here's how you should approach the task:\n",
    "    1. When receiving a task: If the task involves generating data from a chart, use the pre-trained model google/deplot via the Pix2StructProcessor and Pix2StructForConditionalGeneration from the Hugging Face transformers library. The task is to take an image of a chart, process it, and output the underlying data as a table. You are expected to generate the Python code that uses this model to perform the task.\n",
    "    2. When gathering information: You will load the image using Python's PIL.Image.open() and process it using Pix2StructProcessor. You should also prepare a description (e.g., \"Generate underlying data table of the figure below:\") to pass along with the image.\n",
    "    3. When solving the task: Generate the Python code to load the model and processor, process the image, and use the model to generate the underlying data table. After obtaining the string returned from the model with max_new_tokens=512 (important), process the string further by data_string = data_string.replace('<0x0A>', '\\n'); lines = data_string.split('\\n'); has_title = \"TITLE\" in data_string; header_lines = [line.split(' | ') for line in lines if '|' in line][:2 if has_title else 1]; header_length = len(header_lines[-1]); lines = lines[2:] if has_title else lines[1:]; data = []; for line in lines:try:parts = line.split(' | ');data.append(parts);except ValueError:;pass; df = pd.DataFrame(data, columns=header_lines[-1]). Do not use raise, if-else, or try-catch to handle errors. The code should decode the model's output and print the generated data.\n",
    "    4. Error handling: If there is an error in the code, provide a corrected version. Ensure the code works efficiently for the task at hand.\n",
    "    5. Task completion: After completing the task and generating the required Python code. Your goal is to generate efficient Python code that leverages the google/deplot model to convert charts into tabular data. The code you generate should be complete, functional, and ready to execute. After generating the Python code, close the ```.\n",
    "    6. Respond with \"TERMINATE\" to indicate that everything is done.\n",
    "    \"\"\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    "    code_execution_config=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FactCheckAgent (Agent kiểm tra một đoạn thông tin về biểu đồ) (Chart Fact-Checking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:18:20.789092Z",
     "iopub.status.busy": "2024-12-17T15:18:20.788759Z",
     "iopub.status.idle": "2024-12-17T15:18:20.804355Z",
     "shell.execute_reply": "2024-12-17T15:18:20.803525Z",
     "shell.execute_reply.started": "2024-12-17T15:18:20.789065Z"
    }
   },
   "outputs": [],
   "source": [
    "fact_checking_agent = AssistantAgent(\n",
    "    name=\"FactCheckAgent\",\n",
    "    system_message=\"\"\"You are a senior Python engineer. Your task is to answer factual questions based on the data or chart images provided. You will generate Python code to analyze charts, images, or other formats to extract the answers. Here are the steps you need to follow:\n",
    "    1. Load the pre-trained model \"khhuang/chartve\" VisionEncoderDecoderModel and DonutProcessor\n",
    "    2. When solving the task: - Generate the necessary Python code to solve the task.def format_query(sentence):return f\"Does the image entails this statement: \\\"{sentence}\\\"?\";query = format_query(CAPTION_SENTENCE);img = Image.open(\"chart_example_1.png\");pixel_values = processor(img.convert(\"RGB\"), random_padding=False, return_tensors=\"pt\").pixel_values;pixel_values = pixel_values.cuda();decoder_input_ids = processor.tokenizer(query, add_special_tokens=False, return_tensors=\"pt\", max_length=510).input_ids.cuda();outputs = model(pixel_values, decoder_input_ids=decoder_input_ids);binary_entail_prob_positive = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1].item(). Print output is output positive - Ensure the code is complete and ready to execute without requiring modifications from the user. Do not use raise, if-else, or try-catch to handle errors.\n",
    "    3. Error handling: - If an error occurs, provide an updated script to fix the issue. Make sure to handle edge cases or potential issues in the data processing.\n",
    "    4. Task completion: After generating the Python code, close the ```. After completing the task and generating the required Python code. Your goal is to generate efficient Python code that leverages the google/deplot model to convert charts into tabular data. The code you generate should be complete, functional, and ready to execute.\n",
    "    5. Respond with \"TERMINATE\" to indicate that everything is done (outside code python).\n",
    "    \"\"\",\n",
    "    llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0},\n",
    "    code_execution_config=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executor (Agent chạy code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.965298Z",
     "iopub.status.busy": "2024-12-17T15:07:31.965059Z",
     "iopub.status.idle": "2024-12-17T15:07:31.971377Z",
     "shell.execute_reply": "2024-12-17T15:07:31.970526Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.965277Z"
    }
   },
   "outputs": [],
   "source": [
    "executor = UserProxyAgent(\n",
    "    name=\"Executor\",\n",
    "    system_message=\"Executor. Execute the code written by the engineer and report the result.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 3,\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hàm điều chỉnh người nói trong group chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:07:31.972877Z",
     "iopub.status.busy": "2024-12-17T15:07:31.972526Z",
     "iopub.status.idle": "2024-12-17T15:07:31.983256Z",
     "shell.execute_reply": "2024-12-17T15:07:31.982543Z",
     "shell.execute_reply.started": "2024-12-17T15:07:31.972842Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_speaker_selection_func(last_speaker: Agent, groupchat: GroupChat):\n",
    "    \"\"\"Define a customized speaker selection function.\n",
    "    A recommended way is to define a transition for each speaker in the groupchat.\n",
    "\n",
    "    Returns:\n",
    "        Return an `Agent` class or a string from ['auto', 'manual', 'random', 'round_robin'] to select a default method to use.\n",
    "    \"\"\"\n",
    "    messages = groupchat.messages\n",
    "\n",
    "    if len(messages) <= 1:\n",
    "        return planner\n",
    "    \n",
    "    if last_speaker is planner:\n",
    "        if \"Chart-to-Table Conversion\" in messages[-1][\"content\"]:\n",
    "            return conversion_agent\n",
    "        elif \"Chart Question Answering\" in messages[-1][\"content\"]:\n",
    "            return qa_agent\n",
    "        elif \"Chart Fact-Checking\" in messages[-1][\"content\"]:\n",
    "            return fact_checking_agent\n",
    "        elif \"None\" in  messages[-1][\"content\"]:\n",
    "            print(messages[-4][\"name\"])\n",
    "            listAgent = [conversion_agent, qa_agent, fact_checking_agent]\n",
    "            for agent in listAgent:\n",
    "                if(messages[-4][\"name\"] == agent.name):\n",
    "                    return agent\n",
    "\n",
    "    elif last_speaker is conversion_agent:\n",
    "        if \"```python\" in messages[-1][\"content\"]:\n",
    "            return executor\n",
    "        else:\n",
    "            return conversion_agent\n",
    "        \n",
    "    elif last_speaker is qa_agent:\n",
    "        if \"```python\" in messages[-1][\"content\"]:\n",
    "            return executor\n",
    "        else:\n",
    "            return qa_agent\n",
    "        \n",
    "    elif last_speaker is fact_checking_agent:\n",
    "        if \"```python\" in messages[-1][\"content\"]:\n",
    "            return executor\n",
    "        else:\n",
    "            return fact_checking_agent\n",
    "        \n",
    "    elif last_speaker is executor:\n",
    "        if \"exitcode: 1\" in messages[-1][\"content\"] or \"Error\" in messages[-1][\"content\"]:\n",
    "            if \"Chart-to-Table Conversion\" in messages[1][\"content\"]:\n",
    "                return conversion_agent\n",
    "            elif \"Chart Question Answering\" in messages[1][\"content\"]:\n",
    "                return qa_agent\n",
    "            elif \"Chart Fact-Checking\" in messages[1][\"content\"]:\n",
    "                return fact_checking_agent\n",
    "        else:\n",
    "            return boss\n",
    "    elif last_speaker is boss:\n",
    "        return planner\n",
    "    else: \n",
    "        return \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tạo GroupChat giữa các Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:18:24.719222Z",
     "iopub.status.busy": "2024-12-17T15:18:24.718922Z",
     "iopub.status.idle": "2024-12-17T15:18:24.735297Z",
     "shell.execute_reply": "2024-12-17T15:18:24.734475Z",
     "shell.execute_reply.started": "2024-12-17T15:18:24.719197Z"
    }
   },
   "outputs": [],
   "source": [
    "groupchat = GroupChat(\n",
    "    agents=[boss, planner, qa_agent, conversion_agent, fact_checking_agent, executor],\n",
    "    messages=[],\n",
    "    max_round=20,\n",
    "    speaker_selection_method=custom_speaker_selection_func,\n",
    ")\n",
    "\n",
    "manager = GroupChatManager(groupchat=groupchat, llm_config={\"config_list\": config_list, \"timeout\": 60, \"temperature\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Chart-to-Table Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:07:24.972049Z",
     "iopub.status.busy": "2024-12-17T14:07:24.971374Z",
     "iopub.status.idle": "2024-12-17T14:09:12.341190Z",
     "shell.execute_reply": "2024-12-17T14:09:12.340351Z",
     "shell.execute_reply.started": "2024-12-17T14:07:24.972014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBoss\u001b[0m (to chat_manager):\n",
      "\n",
      "Use image in '/kaggle/working/chart_example_1.png' chart_example_1.png and generate underlying data table of the figure\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:07:25] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Chart-to-Table Conversion\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ConversionAgent\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:07:28] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mConversionAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "from PIL import Image\n",
      "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
      "import pandas as pd\n",
      "\n",
      "# Load the image\n",
      "image = Image.open(\"/kaggle/working/chart_example_1.png\")\n",
      "\n",
      "# Load the pre-trained model and processor\n",
      "processor = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n",
      "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\")\n",
      "\n",
      "# Prepare the input\n",
      "inputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\n",
      "\n",
      "# Generate the data table\n",
      "out = model.generate(**inputs, max_new_tokens=512)\n",
      "data_string = processor.decode(out[0], skip_special_tokens=True)\n",
      "data_string = data_string.replace('<0x0A>', '\\n')\n",
      "lines = data_string.split('\\n')\n",
      "has_title = \"TITLE\" in data_string\n",
      "header_lines = [line.split(' | ') for line in lines if '|' in line][:2 if has_title else 1]\n",
      "header_length = len(header_lines[-1])\n",
      "lines = lines[2:] if has_title else lines[1:]\n",
      "data = []\n",
      "for line in lines:\n",
      "    parts = line.split(' | ')\n",
      "    data.append(parts)\n",
      "\n",
      "df = pd.DataFrame(data, columns=header_lines[-1])\n",
      "print(df)\n",
      "\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "  Characteristic Facebook Messenger WhatsApp \n",
      "0          18-29                73%      30% \n",
      "1          30-59                66%      25% \n",
      "2            60+                43%        6%\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Boss\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as Boss. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  save to data.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBoss\u001b[0m (to chat_manager):\n",
      "\n",
      "save to data.csv\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:08:21] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Chart-to-Table Conversion\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ConversionAgent\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:08:24] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mConversionAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "from PIL import Image\n",
      "from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration\n",
      "import pandas as pd\n",
      "\n",
      "# Load the image\n",
      "image = Image.open(\"/kaggle/working/chart_example_1.png\")\n",
      "\n",
      "# Load the pre-trained model and processor\n",
      "processor = Pix2StructProcessor.from_pretrained(\"google/deplot\")\n",
      "model = Pix2StructForConditionalGeneration.from_pretrained(\"google/deplot\")\n",
      "\n",
      "# Prepare the input\n",
      "inputs = processor(images=image, text=\"Generate underlying data table of the figure below:\", return_tensors=\"pt\")\n",
      "\n",
      "# Generate the data table\n",
      "out = model.generate(**inputs, max_new_tokens=512)\n",
      "data_string = processor.decode(out[0], skip_special_tokens=True)\n",
      "data_string = data_string.replace('<0x0A>', '\\n')\n",
      "lines = data_string.split('\\n')\n",
      "has_title = \"TITLE\" in data_string\n",
      "header_lines = [line.split(' | ') for line in lines if '|' in line][:2 if has_title else 1]\n",
      "header_length = len(header_lines[-1])\n",
      "lines = lines[2:] if has_title else lines[1:]\n",
      "data = []\n",
      "for line in lines:\n",
      "    parts = line.split(' | ')\n",
      "    data.append(parts)\n",
      "\n",
      "df = pd.DataFrame(data, columns=header_lines[-1])\n",
      "\n",
      "# Save to CSV\n",
      "df.to_csv(\"data.csv\", index=False)\n",
      "\n",
      "print(\"Data saved to data.csv\")\n",
      "\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Data saved to data.csv\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Boss\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as Boss. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"Use image in '/kaggle/working/chart_example_1.png' chart_example_1.png and generate underlying data table of the figure\"\n",
    "boss.initiate_chat(\n",
    "    manager,\n",
    "    message=message,\n",
    ")\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Chart Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T14:38:28.719431Z",
     "iopub.status.busy": "2024-12-17T14:38:28.718749Z",
     "iopub.status.idle": "2024-12-17T14:40:49.934095Z",
     "shell.execute_reply": "2024-12-17T14:40:49.933188Z",
     "shell.execute_reply.started": "2024-12-17T14:38:28.719398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBoss\u001b[0m (to chat_manager):\n",
      "\n",
      "Use image in '/kaggle/working/chart_example_1.png' chart_example_1.png and answer the question: What is the share of respondants who prefer Whatsapp in the 30-59 age group?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Chart Question Answering\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: QAAgent\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:38:32] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mQAAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "from PIL import Image\n",
      "import torch\n",
      "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
      "\n",
      "# Load the pre-trained model\n",
      "model = LlavaForConditionalGeneration.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\", torch_dtype=torch.float16)\n",
      "processor = AutoProcessor.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\")\n",
      "\n",
      "# Prepare the device (GPU if available, otherwise CPU)\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "model.to(device)\n",
      "\n",
      "# Load the image\n",
      "image_path = \"/kaggle/working/chart_example_1.png\"\n",
      "image = Image.open(image_path).convert(\"RGB\")\n",
      "\n",
      "# Input question\n",
      "input_text = \"What is the share of respondants who prefer Whatsapp in the 30-59 age group?\"\n",
      "\n",
      "# Prepare the input prompt\n",
      "input_prompt = f\"<image>\\nQuestion: {input_text} Answer: \"\n",
      "\n",
      "# Process the input\n",
      "inputs = processor(text=input_prompt, images=image, return_tensors=\"pt\")\n",
      "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
      "inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
      "prompt_length = inputs['input_ids'].shape[1]\n",
      "\n",
      "# Generate the answer using beam search\n",
      "generate_ids = model.generate(**inputs, num_beams=4, max_new_tokens=512)\n",
      "\n",
      "# Decode the output\n",
      "output_text = processor.batch_decode(generate_ids[:, prompt_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "\n",
      "# Print the result\n",
      "print(output_text)\n",
      "\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "25% \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Boss\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as Boss. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  What is the share of responders who prefer Facebook Messenger in the 18-29 age group?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBoss\u001b[0m (to chat_manager):\n",
      "\n",
      "What is the share of responders who prefer Facebook Messenger in the 18-29 age group?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:39:49] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Chart Question Answering\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: QAAgent\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 14:39:53] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mQAAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "from PIL import Image\n",
      "import torch\n",
      "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
      "\n",
      "# Load the pre-trained model\n",
      "model = LlavaForConditionalGeneration.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\", torch_dtype=torch.float16)\n",
      "processor = AutoProcessor.from_pretrained(\"ahmed-masry/ChartInstruct-LLama2\")\n",
      "\n",
      "# Prepare the device (GPU if available, otherwise CPU)\n",
      "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "model.to(device)\n",
      "\n",
      "# Load the image\n",
      "image_path = \"/kaggle/working/chart_example_1.png\"\n",
      "image = Image.open(image_path).convert(\"RGB\")\n",
      "\n",
      "# Input question\n",
      "input_text = \"What is the share of responders who prefer Facebook Messenger in the 18-29 age group?\"\n",
      "\n",
      "# Prepare the input prompt\n",
      "input_prompt = f\"<image>\\nQuestion: {input_text} Answer: \"\n",
      "\n",
      "# Process the input\n",
      "inputs = processor(text=input_prompt, images=image, return_tensors=\"pt\")\n",
      "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
      "inputs['pixel_values'] = inputs['pixel_values'].to(torch.float16)\n",
      "prompt_length = inputs['input_ids'].shape[1]\n",
      "\n",
      "# Generate the answer using beam search\n",
      "generate_ids = model.generate(**inputs, num_beams=4, max_new_tokens=512)\n",
      "\n",
      "# Decode the output\n",
      "output_text = processor.batch_decode(generate_ids[:, prompt_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
      "\n",
      "# Print the result\n",
      "print(output_text)\n",
      "\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "73% \n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Boss\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as Boss. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"Use image in '/kaggle/working/chart_example_1.png' chart_example_1.png and answer the question: What is the share of respondants who prefer Whatsapp in the 30-59 age group?\"\n",
    "boss.initiate_chat(\n",
    "    manager,\n",
    "    message=message,\n",
    ")\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Chart Fact-Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T15:18:30.450834Z",
     "iopub.status.busy": "2024-12-17T15:18:30.450460Z",
     "iopub.status.idle": "2024-12-17T15:19:44.546897Z",
     "shell.execute_reply": "2024-12-17T15:19:44.546095Z",
     "shell.execute_reply.started": "2024-12-17T15:18:30.450803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBoss\u001b[0m (to chat_manager):\n",
      "\n",
      "Use image in '/kaggle/working/chart_example_1.png' chart_example_1.png and caption: Share of Facebook Messenger users in the United States as of January 2018, by age group. Check if the caption matches the chart or not?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Chart Fact-Checking\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: FactCheckAgent\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 15:18:32] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mFactCheckAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "from PIL import Image\n",
      "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
      "import torch\n",
      "\n",
      "# Load the pre-trained model and processor\n",
      "processor = DonutProcessor.from_pretrained(\"khhuang/chartve\")\n",
      "model = VisionEncoderDecoderModel.from_pretrained(\"khhuang/chartve\").cuda()\n",
      "\n",
      "def format_query(sentence):\n",
      "    return f\"Does the image entails this statement: \\\"{sentence}\\\"?\"\n",
      "\n",
      "CAPTION_SENTENCE = \"Share of Facebook Messenger users in the United States as of January 2018, by age group.\"\n",
      "query = format_query(CAPTION_SENTENCE)\n",
      "\n",
      "img = Image.open(\"/kaggle/working/chart_example_1.png\")\n",
      "pixel_values = processor(img.convert(\"RGB\"), random_padding=False, return_tensors=\"pt\").pixel_values\n",
      "pixel_values = pixel_values.cuda()\n",
      "decoder_input_ids = processor.tokenizer(query, add_special_tokens=False, return_tensors=\"pt\", max_length=510).input_ids.cuda()\n",
      "\n",
      "outputs = model(pixel_values, decoder_input_ids=decoder_input_ids)\n",
      "binary_entail_prob_positive = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1].item()\n",
      "\n",
      "print(f\"Probability that the caption matches the chart: {binary_entail_prob_positive}\")\n",
      "\n",
      "```\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Probability that the caption matches the chart: 0.9958046078681946\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Boss\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as Boss. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  change caption to a chart of NVDA and TESLA stock price change YTD and check \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mBoss\u001b[0m (to chat_manager):\n",
      "\n",
      "change caption to a chart of NVDA and TESLA stock price change YTD and check \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 15:19:16] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Chart Fact-Checking\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: FactCheckAgent\n",
      "\u001b[0m\n",
      "[autogen.oai.client: 12-17 15:19:19] {351} WARNING - Model gemini-1.5-flash-latest is not found. The cost will be 0. In your config_list, add field {\"price\" : [prompt_price_per_1k, completion_token_price_per_1k]} for customized pricing.\n",
      "\u001b[33mFactCheckAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "from PIL import Image\n",
      "from transformers import VisionEncoderDecoderModel, DonutProcessor\n",
      "import torch\n",
      "\n",
      "# Load the pre-trained model and processor\n",
      "processor = DonutProcessor.from_pretrained(\"khhuang/chartve\")\n",
      "model = VisionEncoderDecoderModel.from_pretrained(\"khhuang/chartve\").cuda()\n",
      "\n",
      "def format_query(sentence):\n",
      "    return f\"Does the image entails this statement: \\\"{sentence}\\\"?\"\n",
      "\n",
      "CAPTION_SENTENCE = \"A chart of NVDA and TESLA stock price change YTD\"\n",
      "query = format_query(CAPTION_SENTENCE)\n",
      "\n",
      "img = Image.open(\"/kaggle/working/chart_example_1.png\") #This needs to be changed to your image file.  This code assumes the image is in the same directory.\n",
      "pixel_values = processor(img.convert(\"RGB\"), random_padding=False, return_tensors=\"pt\").pixel_values\n",
      "pixel_values = pixel_values.cuda()\n",
      "decoder_input_ids = processor.tokenizer(query, add_special_tokens=False, return_tensors=\"pt\", max_length=510).input_ids.cuda()\n",
      "\n",
      "outputs = model(pixel_values, decoder_input_ids=decoder_input_ids)\n",
      "binary_entail_prob_positive = torch.nn.functional.softmax(outputs['logits'].squeeze()[-1,[2334, 49922]])[1].item()\n",
      "\n",
      "print(f\"Probability that the caption matches the chart: {binary_entail_prob_positive}\")\n",
      "\n",
      "```\n",
      "\n",
      "**Before running:**  Replace `/kaggle/working/chart_example_1.png` with the actual path to your image file containing the NVDA and TESLA stock price chart.  The code assumes you have the image file in a location accessible to your Python environment.  If the image is not a chart showing NVDA and TESLA stock prices YTD, the probability will likely be low.\n",
      "TERMINATE\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Executor\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mExecutor\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Probability that the caption matches the chart: 0.04390009492635727\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Boss\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replying as Boss. Provide feedback to chat_manager. Press enter to skip and use auto-reply, or type 'exit' to end the conversation:  exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Done'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = \"Use image in '/kaggle/working/chart_example_1.png' chart_example_1.png and caption: Share of Facebook Messenger users in the United States as of January 2018, by age group. Check if the caption matches the chart or not?\"\n",
    "boss.initiate_chat(\n",
    "    manager,\n",
    "    message=message,\n",
    ")\n",
    "\"Done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
